● Activation function: D ecides whether the neuron “fires” (outputs 1) or not (outputs 
0). 
Mathematically: 
y=f(∑i=1nwixi+--pppl,,,,,,,,,,,)y = f\left(\sum_{i=1}^n w_i x_i + b\right)y=f(i=1∑nwixi+b) 
where fff = activation function (like step, sigmoid, ReLU, etc.). 
🔹 Example 
Imagine you want a perceptron to decide: 
● Input: Weather (sunny=1, rainy=0), Weekend (yes=1, no=0). 
● Output: Go for a picnic (Yes=1, No=0). 
The perceptron will learn weights for "sunny" and "weekend" to predict picnic plans. 
🔹 Limitations 
● A single perceptron can only solve linearly separable problems (like AND, OR 
logic gates). 
● It cannot solve problems like XOR (non-linear). 
This limitation is why multi-layer perceptrons (MLPs) (with hidden layers) were 
introduced → leading to deep learning 
.How it works 
When we would use it 
Real-world scenario (no numeric plugging) 
Drawback compared to the linear formula 
--- 
1. Linear Weighted Sum 
Formula: 
z = w_1x_1 + w_2x_2 + \dots + b 
How it works: 
Each input is multiplied by a weight , added together, and offset by a bias . 
When we use it: 
When each factor contributes independently and proportionally to the decision. 
Real-world example: 
Predicting student performance using study hours, attendance rate, and assignment quality — 
each adds to the total performance score. 
Drawback: 
Linear by itself can’t capture complex non-linear interactions — but in neural networks we 
solve this by applying non-linear activations after. 
--- 
2. Multiplication (Product Model) 
Formula: 
z = x_1^{w_1} \times x_2^{w_2} \times \dots 
How it works: 
Inputs are multiplied together, often after raising to weights. 
When we use it: 
When all inputs must work together and the absence of one ruins the outcome. 
Real-world example: 
Modeling team performance where success depends on all departments functioning — if one 
department fails (value ≈ 0), the whole output drops. 
Drawback 
Too harsh — one small or zero input kills the output completely. No partial credit. 
--- 
3. Max Function 
Formula: 
z = \max(w_1x_1, w_2x_2, \dots) 
How it works: 
Only the strongest weighted input is used for the decision. 
When we use it: 
When only the most dominant factor matters and others are irrelevant. 
Real-world example: 
Emergency room triage — patient’s most severe symptom determines priority; lesser 
symptoms are ignored. 
Drawback: 
Ignores useful smaller signals — not great for most predictive tasks. 
--- 
4. Polynomial Combination 
Formula: 
z = w_1x_1 + w_2x_2 + w_{12}x_1x_2 + w_{11}x_1^2 + \dots 
How it works: 
Includes original inputs plus their squares, products, etc., to model interactions and curves. 
When we use it: 
When there’s a clear interaction between variables or a known curve-shaped relationship. 
Real-world example: 
Crop yield prediction — temperature and rainfall not only matter individually but together 
(too much rain and high heat cause diseases). 
Drawback: 
Can blow up with large inputs, and needs more data to avoid overfitting. 
--- 
5. Logical Rule (AND / OR) 
Formula (AND): 
z = (x_1 > \text{threshold}) \land (x_2 > \text{threshold}) 
How it works: 
Outputs 1 only if all conditions are true; otherwise 0. 
When we use it: 
When the task is strictly logical, like password verification. 
Real-world example: 
User login — correct username AND correct password needed. 
Drawback: 
No smooth learning — one wrong bit means total failure; not differentiable for gradient 
descent. 
--- 
Why Linear Weighted Sum Is “Best by Default” 
General: All the others can be expressed as a special case of the linear model after applying a 
transformation to the inputs. 
Flexible: Can approximate any of the others if needed, by adjusting weights and activation 
functions. 
Learnable: Works well with gradient descent because it’s differentiable everywhere. 
Fair: Gives all inputs a chance to influence the output — no instant elimination unless 
weights dictate it. 
🔹1. What is an Activation Function? 
It decides the output of a neuron after calculating the weighted sum. 
● Without it → the network is just linear (like plain regression). 
● With it → the network can learn non-linear patterns (images, text, etc.). 
● During training, activation functions also allow gradient descent by providing 
gradients (derivatives) for backpropagation. 
🔹2. Types of Activation Functions 
(a) Step Function 
f(x)={1if x≥00if x<0f(x) = \begin{cases} 1 & \text{if } x \geq 0 \\ 0 & \text{if } x < 0 
\end{cases}f(x)={10if x≥0if x<0 
● Oldest activation (used in Perceptron). 
● Only outputs 0 or 1. 
● ❌ No gradient → cannot use gradient descent. 
● Used in early perceptrons, not modern DL 
(b) Sigmoid Function 
f(x)=11+e−xf(x) = \frac{1}{1 + e^{-x}}f(x)=1+e−x1 
● Squashes values into (0,1). 
● Good for probabilities. 
● Derivative (gradient): 
f′(x)=f(x)(1−f(x))f'(x) = f(x)(1 - f(x))f′(x)=f(x)(1−f(x)) 
● ❌ Problem: Vanishing gradient (for very large +x or -x, gradient ≈ 0). 
(c) Tanh Function 
f(x)=tanh(x)=ex−e−xex+e−xf(x) = \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}f(x)=tanh(x)=ex+e−xex−e−x 
● Outputs in (-1,1) (centered around 0 → better than sigmoid). 
● Gradient: 
f′(x)=1−tanh2(x)f'(x) = 1 - \tanh^2(x)f′(x)=1−tanh2(x) 
● ❌ Still suffers from vanishing gradient at extremes. 
(d) ReLU (Rectified Linear Unit) 
f(x)=max(0,x)f(x) = \max(0, x)f(x)=max(0,x) 
● If input > 0 → output = x 
● If input ≤ 0 → output = 0 
● Gradient: 
f′(x)={1x>00x≤0f'(x) = \begin{cases} 1 & x > 0 \\ 0 & x \leq 0 \end{cases}f′(x)={10x>0x≤0 
● ✅ Solves vanishing gradient (fast training). 
● ❌ Problem: Dying ReLU (neurons stuck at 0). 
(e) Leaky ReLU 
f(x)={xx>00.01xx≤0f(x) = \begin{cases} x & x > 0 \\ 0.01x & x \leq 0 \end{cases}f(x)={x0.01xx>0x≤0 
● Small slope for negative side → prevents dying ReLU. 
● Gradient is never exactly 0. 
(f) Softmax 
f(xi)=exi∑jexjf(x_i) = \frac{e^{x_i}}{\sum_j e^{x_j}}f(xi)=∑jexjexi 
● Used in final layer for multi-class classification. 
● Converts raw scores → probabilities that sum to 1. 
🔹3. Gradient Descent with Activation 
● Forward pass: compute neuron output using activation. 
where fff = activation function (like step, sigmoid, ReLU, etc.). 
🔹 Example 
Imagine you want a perceptron to decide: 
● Input: Weather (sunny=1, rainy=0), Weekend (yes=1, no=0). 
● Output: Go for a picnic (Yes=1, No=0). 
The perceptron will learn weights for "sunny" and "weekend" to predict picnic plans. 
🔹 Limitations 
● A single perceptron can only solve linearly separable problems (like AND, OR 
logic gates). 
● It cannot solve problems like XOR (non-linear). 
This limitation is why multi-layer perceptrons (MLPs) (with hidden layers) were 
introduced → leading to deep learning 
.How it works 
When we would use it 
Real-world scenario (no numeric plugging) 
Drawback compared to the linear formula 
--- 
1. Linear Weighted Sum 
Formula: 
z = w_1x_1 + w_2x_2 + \dots + b 
How it works: 
Each input is multiplied by a weight , added together, and offset by a bias . 
When we use it: 
When each factor contributes independently and proportionally to the decision. 
Real-world example: 
Predicting student performance using study hours, attendance rate, and assignment quality — 
each adds to the total performance score. 
Drawback: 
Linear by itself can’t capture complex non-linear interactions — but in neural networks we 
solve this by applying non-linear activations after. 
--- 
2. Multiplication (Product Model) 
Formula: 
z = x_1^{w_1} \times x_2^{w_2} \times \dots 
How it works: 
Inputs are multiplied together, often after raising to weights. 
When we use it: 
When all inputs must work together and the absence of one ruins the outcome. 
Real-world example: 
Modeling team performance where success depends on all departments functioning — if one 
department fails (value ≈ 0), the whole output drops. 
Drawback 
Too harsh — one small or zero input kills the output completely. No partial credit. 
--- 
3. Max Function 
Formula: 
z = \max(w_1x_1, w_2x_2, \dots) 
How it works: 
Only the strongest weighted input is used for the decision. 
When we use it: 
Formula: 
z = \max(w_1x_1, w_2x_2, \dots) 
How it works: 
Only the strongest weighted input is used for the decision. 
When we use it: 
When only the most dominant factor matters and others are irrelevant. 
Real-world example: 
Emergency room triage — patient’s most severe symptom determines priority; lesser 
symptoms are ignored. 
Drawback: 
Ignores useful smaller signals — not great for most predictive tasks. 
--- 
4. Polynomial Combination 
Formula: 
z = w_1x_1 + w_2x_2 + w_{12}x_1x_2 + w_{11}x_1^2 + \dots 
How it works: 
Includes original inputs plus their squares, products, etc., to model interactions and curves. 
When we use it: 
When there’s a clear interaction between variables or a known curve-shaped relationship. 
Real-world example: 
Crop yield prediction — temperature and rainfall not only matter individually but together 
(too much rain and high heat cause diseases). 
Drawback: 
Can blow up with large inputs, and needs more data to avoid overfitting. 
--- 
5. Logical Rule (AND / OR) 
Formula (AND): 
z = (x_1 > \text{threshold}) \land (x_2 > \text{threshold}) 
How it works: 
Outputs 1 only if all conditions are true; otherwise 0. 
When we use it: 
When the task is strictly logical, like password verification. 
Real-world example: 
User login — correct username AND correct password needed. 
Drawback: 
No smooth learning — one wrong bit means total failure; not differentiable for gradient 
descent. 
--- 
Why Linear Weighted Sum Is “Best by Default” 
General: All the others can be expressed as a special case of the linear model after applying a 
transformation to the inputs. 
Flexible: Can approximate any of the others if needed, by adjusting weights and activation 
functions. 
Learnable: Works well with gradient descent because it’s differentiable everywhere. 
Fair: Gives all inputs a chance to influence the output — no instant elimination unless 
weights dictate it. 
🔹1. What is an Activation Function? 
It decides the output of a neuron after calculating the weighted sum. 
● Without it → the network is just linear (like plain regression). 
● With it → the network can learn non-linear patterns (images, text, etc.). 
● During training, activation functions also allow gradient descent by providing 
gradients (derivatives) for backpropagation. 
🔹2. Types of Activation Functions 
(a) Step Function 
f(x)={1if x≥00if x<0f(x) = \begin{cases} 1 & \text{if } x \geq 0 \\ 0 & \text{if } x < 0 
\end{cases}f(x)={10if x≥0if x<0 
● Oldest activation (used in Perceptron). 
● Only outputs 0 or 1. 
● ❌ No gradient → cannot use gradient descent. 
● Used in early perceptrons, not modern DL 
(b) Sigmoid Function 
f(x)=11+e−xf(x) = \frac{1}{1 + e^{-x}}f(x)=1+e−x1 
● Squashes values into (0,1). 
● Good for probabilities. 
● Derivative (gradient): 
f′(x)=f(x)(1−f(x))f'(x) = f(x)(1 - f(x))f′(x)=f(x)(1−f(x)) 
● ❌ Problem: Vanishing gradient (for very large +x or -x, gradient ≈ 0). 
(c) Tanh Function 
f(x)=tanh(x)=ex−e−xex+e−xf(x) = \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}f(x)=tanh(x)=ex+e−xex−e−x 
● Outputs in (-1,1) (centered around 0 → better than sigmoid). 
