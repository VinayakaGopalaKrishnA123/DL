● Activation function: D ecides whether the neuron “fires” (outputs 1) or not (outputs 
0). 
Mathematically: 
y=f(∑i=1nwixi+--pppl,,,,,,,,,,,)y = f\left(\sum_{i=1}^n w_i x_i + b\right)y=f(i=1∑nwixi+b) 
where fff = activation function (like step, sigmoid, ReLU, etc.). 
🔹 Example 
Imagine you want a perceptron to decide: 
● Input: Weather (sunny=1, rainy=0), Weekend (yes=1, no=0). 
● Output: Go for a picnic (Yes=1, No=0). 
The perceptron will learn weights for "sunny" and "weekend" to predict picnic plans. 
🔹 Limitations 
● A single perceptron can only solve linearly separable problems (like AND, OR 
logic gates). 
● It cannot solve problems like XOR (non-linear). 
This limitation is why multi-layer perceptrons (MLPs) (with hidden layers) were 
introduced → leading to deep learning 
.How it works 
When we would use it 
Real-world scenario (no numeric plugging) 
Drawback compared to the linear formula 
--- 
1. Linear Weighted Sum 
Formula: 
z = w_1x_1 + w_2x_2 + \dots + b 
How it works: 
Each input is multiplied by a weight , added together, and offset by a bias . 
When we use it: 
When each factor contributes independently and proportionally to the decision. 
Real-world example: 
Predicting student performance using study hours, attendance rate, and assignment quality — 
each adds to the total performance score. 
Drawback: 
Linear by itself can’t capture complex non-linear interactions — but in neural networks we 
solve this by applying non-linear activations after. 
--- 
2. Multiplication (Product Model) 
Formula: 
z = x_1^{w_1} \times x_2^{w_2} \times \dots 
How it works: 
Inputs are multiplied together, often after raising to weights. 
When we use it: 
When all inputs must work together and the absence of one ruins the outcome. 
Real-world example: 
Modeling team performance where success depends on all departments functioning — if one 
department fails (value ≈ 0), the whole output drops. 
Drawback 
Too harsh — one small or zero input kills the output completely. No partial credit. 
--- 
3. Max Function 
Formula: 
z = \max(w_1x_1, w_2x_2, \dots) 
How it works: 
Only the strongest weighted input is used for the decision. 
When we use it: 
When only the most dominant factor matters and others are irrelevant. 
Real-world example: 
Emergency room triage — patient’s most severe symptom determines priority; lesser 
symptoms are ignored. 
Drawback: 
Ignores useful smaller signals — not great for most predictive tasks. 
--- 
4. Polynomial Combination 
Formula: 
z = w_1x_1 + w_2x_2 + w_{12}x_1x_2 + w_{11}x_1^2 + \dots 
How it works: 
Includes original inputs plus their squares, products, etc., to model interactions and curves. 
When we use it: 
When there’s a clear interaction between variables or a known curve-shaped relationship. 
Real-world example: 
Crop yield prediction — temperature and rainfall not only matter individually but together 
(too much rain and high heat cause diseases). 
Drawback: 
Can blow up with large inputs, and needs more data to avoid overfitting. 
--- 
5. Logical Rule (AND / OR) 
Formula (AND): 
z = (x_1 > \text{threshold}) \land (x_2 > \text{threshold}) 
How it works: 
Outputs 1 only if all conditions are true; otherwise 0. 
When we use it: 
When the task is strictly logical, like password verification. 
Real-world example: 
User login — correct username AND correct password needed. 
Drawback: 
No smooth learning — one wrong bit means total failure; not differentiable for gradient 
descent. 
--- 
Why Linear Weighted Sum Is “Best by Default” 
General: All the others can be expressed as a special case of the linear model after applying a 
transformation to the inputs. 
